{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "mlp_pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMwg22DZ6Hmi",
        "outputId": "ddc5a208-616f-46bc-999d-34e23515d3dd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOtiegIu6EVR"
      },
      "source": [
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import joblib\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import roc_auc_score \n",
        "import joblib\n",
        "import pickle\n",
        "from torch.utils.data import DataLoader\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuAH0vGc6EVU"
      },
      "source": [
        "class simple_cnn(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(simple_cnn, self).__init__()\n",
        "    self.cnn_layers = nn.Sequential(\n",
        "        # Defining a 2D convolution layer\n",
        "        nn.Conv2d(1, 3, 4, 2),\n",
        "        nn.BatchNorm2d(3),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool2d(2),\n",
        "        # Defining another 2D convolution layer\n",
        "        nn.Conv2d(3,2,4,2),\n",
        "        nn.BatchNorm2d(2),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.MaxPool2d(2),\n",
        "    )\n",
        "\n",
        "    self.linear_layers = nn.Sequential(\n",
        "        nn.Linear(2 * 7 * 7, 5)\n",
        "    )\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.cnn_layers(x)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = self.linear_layers(x)\n",
        "    return x\n",
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, X, y, indices):\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "    self.indices = indices\n",
        "    \n",
        "  def __len__(self):\n",
        "    #y = self.y\n",
        "    #length = y.shape[1]\n",
        "    return len(self.indices)\n",
        "  \n",
        "  def __getitem__(self, sample_idx):\n",
        "    Id = self.indices[sample_idx]\n",
        "    return {\n",
        "        \"data\": torch.from_numpy(self.X[Id]),\n",
        "        \"label\": self.y[Id]\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4pKOib56EVV",
        "outputId": "588fc6ee-b988-4b65-9496-a7d34fbe873e"
      },
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/trainLabels_cropped.csv\")\n",
        "image_names = df['image']\n",
        "y = df['level']\n",
        "print(Counter(y))\n",
        "\n",
        "X = joblib.load(\"/content/drive/MyDrive/X\")\n",
        "X = X.reshape( (X.shape[0], 128*128) )\n",
        "oversample = SMOTE()\n",
        "X_over, y_over = oversample.fit_resample(X, y)\n",
        "for i in range(X_over.shape[0]):\n",
        "  min = X_over[i].min(axis=0)\n",
        "  max = X_over[i].max(axis=0)\n",
        "  X_std = (X_over[i] - min) / (max - min)\n",
        "  X_over[i] = X_std * (max - min) + min\n",
        "\n",
        "X_over = X_over.reshape((X_over.shape[0], 1, 128, 128) )\n",
        "print(\"oversampled: \",Counter(y_over))\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X_over, y_over, test_size=0.3)\n",
        "indices = np.arange(X_over.shape[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({0: 25802, 2: 5288, 1: 2438, 3: 872, 4: 708})\n",
            "oversampled:  Counter({0: 25802, 1: 25802, 2: 25802, 4: 25802, 3: 25802})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZcJw24l6EVW",
        "outputId": "96ed0e15-a9e2-4b1b-e67d-9cb1c17a066f"
      },
      "source": [
        "X_over = X_over.astype('float')\n",
        "print(X_over.shape)\n",
        "model_cnn = simple_cnn()\n",
        "\n",
        "train_val_indices, heldout_indices = train_test_split(indices, test_size=0.3, random_state=42, stratify=y_over)\n",
        "train_indices, valid_indices = train_test_split(train_val_indices, test_size = 0.2, random_state=42, stratify=y_over[train_val_indices])\n",
        "\n",
        "train_dataset = Dataset(X_over, y_over, train_indices)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_dataset = Dataset(X_over, y_over, valid_indices)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(129010, 1, 128, 128)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIQzsBEG6EVW",
        "outputId": "83f8180d-271d-4f33-c087-8b0a69f71391"
      },
      "source": [
        "epochs = 100\n",
        "device = torch.device('cuda:0')\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model_cnn.parameters(), lr = 0.001)\n",
        "model_cnn = model_cnn.to(device)\n",
        "\n",
        "train_loss_history, valid_loss_history = [], []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  model_cnn.train()\n",
        "  train_loss = []\n",
        "  for batch_idx, batch in enumerate(train_dataloader):\n",
        "    X_batch = batch[\"data\"].to(device)\n",
        "    y_batch = batch[\"label\"].to(device)\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    outputs = model_cnn(X_batch.float())\n",
        "    loss = loss_function(outputs, y_batch)\n",
        "    train_loss.append(loss.item())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  model_cnn.eval()\n",
        "  with torch.no_grad():\n",
        "    valid_loss = []\n",
        "    y_probs_train = torch.empty(0, 5).to(device)\n",
        "    y_true_valid, y_probs_valid, y_pred_valid = [], [], []\n",
        "\n",
        "    for i, batch in enumerate(test_dataloader):\n",
        "      X_batch = batch[\"data\"].to(device)\n",
        "      y_batch = batch[\"label\"].to(device)\n",
        "      \n",
        "      outputs = model_cnn(X_batch.float())\n",
        "      loss = loss_function(outputs, y_batch)\n",
        "      valid_loss.append(loss.item())\n",
        "\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      y_pred_valid += predicted.cpu().numpy().tolist()\n",
        "      y_probs_valid = torch.cat((y_probs_train, outputs), 0)\n",
        "      y_true_valid += y_batch.cpu().numpy().tolist()\n",
        "  y_probs_valid = F.softmax(y_probs_train, dim=1).cpu().numpy()\n",
        "  y_true_valid = np.array(y_true_valid)\n",
        "\n",
        "  train_loss_history.append(np.mean(train_loss))\n",
        "  valid_loss_history.append(np.mean(valid_loss))\n",
        "  \n",
        "  print(f\"Epoch {epoch} train loss: {train_loss_history[-1]}\")\n",
        "  print(f\"Epoch {epoch} test loss: {valid_loss_history[-1]}\")\n",
        "  # print(classification_report(y_true_valid, y_pred_valid))\n",
        "  \n",
        "  state = {\n",
        "      'model_description': str(model_cnn),\n",
        "      'model_state': model_cnn.state_dict(),\n",
        "      'optimizer': optimizer.state_dict()\n",
        "  }\n",
        "  torch.save(state, \"/content/drive/MyDrive/model_cnn.ckpt\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 train loss: 1.3665933989126984\n",
            "Epoch 0 test loss: 1.3172494177262268\n",
            "Epoch 1 train loss: 1.3117160248059525\n",
            "Epoch 1 test loss: 1.3422920274229015\n",
            "Epoch 2 train loss: 1.2966532557278845\n",
            "Epoch 2 test loss: 1.3373362571527596\n",
            "Epoch 3 train loss: 1.2868694776555307\n",
            "Epoch 3 test loss: 1.287922960709346\n",
            "Epoch 4 train loss: 1.2818825599680546\n",
            "Epoch 4 test loss: 1.2767905258037175\n",
            "Epoch 5 train loss: 1.2737662341343396\n",
            "Epoch 5 test loss: 1.2936803869139601\n",
            "Epoch 6 train loss: 1.268997433335513\n",
            "Epoch 6 test loss: 1.2730757103370693\n",
            "Epoch 7 train loss: 1.2642396148904644\n",
            "Epoch 7 test loss: 1.2740894176934718\n",
            "Epoch 8 train loss: 1.2621016021425266\n",
            "Epoch 8 test loss: 1.2722870690662533\n",
            "Epoch 9 train loss: 1.259076784520787\n",
            "Epoch 9 test loss: 1.2602269245120745\n",
            "Epoch 10 train loss: 1.2583755494431124\n",
            "Epoch 10 test loss: 1.257417907141965\n",
            "Epoch 11 train loss: 1.255412296133827\n",
            "Epoch 11 test loss: 1.2806781058597903\n",
            "Epoch 12 train loss: 1.2535667089997824\n",
            "Epoch 12 test loss: 1.2957440720008877\n",
            "Epoch 13 train loss: 1.2521393010846484\n",
            "Epoch 13 test loss: 1.2600877718874928\n",
            "Epoch 14 train loss: 1.2516992886157243\n",
            "Epoch 14 test loss: 1.2680921297612544\n",
            "Epoch 15 train loss: 1.2495377855664127\n",
            "Epoch 15 test loss: 1.2521417932881063\n",
            "Epoch 16 train loss: 1.2492658892915351\n",
            "Epoch 16 test loss: 1.2657005209804846\n",
            "Epoch 17 train loss: 1.247403831160945\n",
            "Epoch 17 test loss: 1.2628344137340046\n",
            "Epoch 18 train loss: 1.2459887439851955\n",
            "Epoch 18 test loss: 1.2472454920253147\n",
            "Epoch 19 train loss: 1.2462577989821522\n",
            "Epoch 19 test loss: 1.2491495634557501\n",
            "Epoch 20 train loss: 1.244595982682082\n",
            "Epoch 20 test loss: 1.2732777283806682\n",
            "Epoch 21 train loss: 1.2448452972436606\n",
            "Epoch 21 test loss: 1.2473996515408843\n",
            "Epoch 22 train loss: 1.2435205293187428\n",
            "Epoch 22 test loss: 1.255150685462008\n",
            "Epoch 23 train loss: 1.243197186580266\n",
            "Epoch 23 test loss: 1.242787263418676\n",
            "Epoch 24 train loss: 1.24182022103596\n",
            "Epoch 24 test loss: 1.258128105541\n",
            "Epoch 25 train loss: 1.242111420610291\n",
            "Epoch 25 test loss: 1.2485596105824932\n",
            "Epoch 26 train loss: 1.241311004195623\n",
            "Epoch 26 test loss: 1.2438463115860632\n",
            "Epoch 27 train loss: 1.241038526920643\n",
            "Epoch 27 test loss: 1.2461923306064133\n",
            "Epoch 28 train loss: 1.2403645136379584\n",
            "Epoch 28 test loss: 1.2796189629146995\n",
            "Epoch 29 train loss: 1.240250084862654\n",
            "Epoch 29 test loss: 1.2547885398561458\n",
            "Epoch 30 train loss: 1.239898685758572\n",
            "Epoch 30 test loss: 1.2413696968934562\n",
            "Epoch 31 train loss: 1.2402818059372205\n",
            "Epoch 31 test loss: 1.2492089831786948\n",
            "Epoch 32 train loss: 1.238030361432328\n",
            "Epoch 32 test loss: 1.2591314560111757\n",
            "Epoch 33 train loss: 1.2383565053779324\n",
            "Epoch 33 test loss: 1.2474249441295124\n",
            "Epoch 34 train loss: 1.237859988972822\n",
            "Epoch 34 test loss: 1.25480686860034\n",
            "Epoch 35 train loss: 1.2384180080267684\n",
            "Epoch 35 test loss: 1.2519783769395243\n",
            "Epoch 36 train loss: 1.2377368069201893\n",
            "Epoch 36 test loss: 1.2776376185905807\n",
            "Epoch 37 train loss: 1.2380227355222135\n",
            "Epoch 37 test loss: 1.2613042584577634\n",
            "Epoch 38 train loss: 1.236908519130771\n",
            "Epoch 38 test loss: 1.2707289246704048\n",
            "Epoch 39 train loss: 1.2355394958921826\n",
            "Epoch 39 test loss: 1.2414223322177522\n",
            "Epoch 40 train loss: 1.235964304478006\n",
            "Epoch 40 test loss: 1.2857883776877035\n",
            "Epoch 41 train loss: 1.236431261361226\n",
            "Epoch 41 test loss: 1.2773092294329047\n",
            "Epoch 42 train loss: 1.2354677412972395\n",
            "Epoch 42 test loss: 1.2392011329057782\n",
            "Epoch 43 train loss: 1.2357944229517075\n",
            "Epoch 43 test loss: 1.2459210677618695\n",
            "Epoch 44 train loss: 1.2348486500783131\n",
            "Epoch 44 test loss: 1.2362953226473643\n",
            "Epoch 45 train loss: 1.2343131427853586\n",
            "Epoch 45 test loss: 1.2370668940325087\n",
            "Epoch 46 train loss: 1.2342518065966157\n",
            "Epoch 46 test loss: 1.2948502946658185\n",
            "Epoch 47 train loss: 1.2343087870531952\n",
            "Epoch 47 test loss: 1.2496989021874148\n",
            "Epoch 48 train loss: 1.2337163220579588\n",
            "Epoch 48 test loss: 1.2454800883788524\n",
            "Epoch 49 train loss: 1.2349523036642556\n",
            "Epoch 49 test loss: 1.2400061262791233\n",
            "Epoch 50 train loss: 1.2344700052634714\n",
            "Epoch 50 test loss: 1.2702672283556773\n",
            "Epoch 51 train loss: 1.2327673252543272\n",
            "Epoch 51 test loss: 1.2573975303569025\n",
            "Epoch 52 train loss: 1.2327050073047567\n",
            "Epoch 52 test loss: 1.2380650447872419\n",
            "Epoch 53 train loss: 1.2327965681166433\n",
            "Epoch 53 test loss: 1.2389095747849967\n",
            "Epoch 54 train loss: 1.23340628995\n",
            "Epoch 54 test loss: 1.2353456530048654\n",
            "Epoch 55 train loss: 1.2330948648672382\n",
            "Epoch 55 test loss: 1.248067596775904\n",
            "Epoch 56 train loss: 1.233255480062233\n",
            "Epoch 56 test loss: 1.266294823097256\n",
            "Epoch 57 train loss: 1.2335894765106317\n",
            "Epoch 57 test loss: 1.2424698175052873\n",
            "Epoch 58 train loss: 1.2327988902692566\n",
            "Epoch 58 test loss: 1.242910453793017\n",
            "Epoch 59 train loss: 1.2336595583215448\n",
            "Epoch 59 test loss: 1.2601738540528098\n",
            "Epoch 60 train loss: 1.2331570729520915\n",
            "Epoch 60 test loss: 1.2390971059512754\n",
            "Epoch 61 train loss: 1.2323910982551778\n",
            "Epoch 61 test loss: 1.23707608367866\n",
            "Epoch 62 train loss: 1.2319655300664945\n",
            "Epoch 62 test loss: 1.2586616530435246\n",
            "Epoch 63 train loss: 1.232514592215907\n",
            "Epoch 63 test loss: 1.2351647990331212\n",
            "Epoch 64 train loss: 1.2332304988000327\n",
            "Epoch 64 test loss: 1.2486889543465927\n",
            "Epoch 65 train loss: 1.23200210973792\n",
            "Epoch 65 test loss: 1.2399691983583538\n",
            "Epoch 66 train loss: 1.232194864507055\n",
            "Epoch 66 test loss: 1.2466788869022059\n",
            "Epoch 67 train loss: 1.2314245333388594\n",
            "Epoch 67 test loss: 1.2345938916341155\n",
            "Epoch 68 train loss: 1.2316554243106352\n",
            "Epoch 68 test loss: 1.2461892932969352\n",
            "Epoch 69 train loss: 1.232588147040064\n",
            "Epoch 69 test loss: 1.2361968654625828\n",
            "Epoch 70 train loss: 1.2321776991612727\n",
            "Epoch 70 test loss: 1.2366426509176462\n",
            "Epoch 71 train loss: 1.2316366283105895\n",
            "Epoch 71 test loss: 1.2381650874976977\n",
            "Epoch 72 train loss: 1.2319059981193239\n",
            "Epoch 72 test loss: 1.2468616547517135\n",
            "Epoch 73 train loss: 1.231287407494943\n",
            "Epoch 73 test loss: 1.2427822482038302\n",
            "Epoch 74 train loss: 1.2316400223965134\n",
            "Epoch 74 test loss: 1.2394929100683636\n",
            "Epoch 75 train loss: 1.232385633317003\n",
            "Epoch 75 test loss: 1.2367596462000385\n",
            "Epoch 76 train loss: 1.2313978178929819\n",
            "Epoch 76 test loss: 1.2367103495783183\n",
            "Epoch 77 train loss: 1.2313751014489005\n",
            "Epoch 77 test loss: 1.239335436281804\n",
            "Epoch 78 train loss: 1.231576650094099\n",
            "Epoch 78 test loss: 1.2396880472506735\n",
            "Epoch 79 train loss: 1.2319096592382797\n",
            "Epoch 79 test loss: 1.2344682100804871\n",
            "Epoch 80 train loss: 1.2313062496371139\n",
            "Epoch 80 test loss: 1.2489104835388938\n",
            "Epoch 81 train loss: 1.2314371970293267\n",
            "Epoch 81 test loss: 1.2388713543069658\n",
            "Epoch 82 train loss: 1.2305627140563815\n",
            "Epoch 82 test loss: 1.2554039394897623\n",
            "Epoch 83 train loss: 1.2315895013623368\n",
            "Epoch 83 test loss: 1.2417069593503702\n",
            "Epoch 84 train loss: 1.2320641196333697\n",
            "Epoch 84 test loss: 1.2574041867003425\n",
            "Epoch 85 train loss: 1.2305364202777298\n",
            "Epoch 85 test loss: 1.2392823818294403\n",
            "Epoch 86 train loss: 1.2315205804429505\n",
            "Epoch 86 test loss: 1.273416827929736\n",
            "Epoch 87 train loss: 1.2311370439292901\n",
            "Epoch 87 test loss: 1.2394946270191207\n",
            "Epoch 88 train loss: 1.2301162103301555\n",
            "Epoch 88 test loss: 1.240256354581341\n",
            "Epoch 89 train loss: 1.2312573094722759\n",
            "Epoch 89 test loss: 1.2359421874524847\n",
            "Epoch 90 train loss: 1.2299158951157063\n",
            "Epoch 90 test loss: 1.2418667798328737\n",
            "Epoch 91 train loss: 1.230080235469014\n",
            "Epoch 91 test loss: 1.2807591054127831\n",
            "Epoch 92 train loss: 1.2314631119040498\n",
            "Epoch 92 test loss: 1.233458690845503\n",
            "Epoch 93 train loss: 1.2307223683442554\n",
            "Epoch 93 test loss: 1.2471991985930992\n",
            "Epoch 94 train loss: 1.2299763114184168\n",
            "Epoch 94 test loss: 1.2391736498991086\n",
            "Epoch 95 train loss: 1.2300419762031076\n",
            "Epoch 95 test loss: 1.236429415827505\n",
            "Epoch 96 train loss: 1.230635054772464\n",
            "Epoch 96 test loss: 1.2353662802136829\n",
            "Epoch 97 train loss: 1.2306701804389988\n",
            "Epoch 97 test loss: 1.2350641288942668\n",
            "Epoch 98 train loss: 1.229583657400285\n",
            "Epoch 98 test loss: 1.2370979587939097\n",
            "Epoch 99 train loss: 1.2292333141272842\n",
            "Epoch 99 test loss: 1.3043759602118719\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIEGHbYrOZ4T",
        "outputId": "b1ab0a55-3c65-4d0c-d05d-fc4d212c4344"
      },
      "source": [
        "heldout_dataset = Dataset(X_over, y_over, heldout_indices)\n",
        "heldout_dataloader = DataLoader(heldout_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "model_cnn.eval()\n",
        "with torch.no_grad():\n",
        "  valid_loss = []\n",
        "  y_probs_train = torch.empty(0, 5).to(device)\n",
        "  y_true_heldout, y_probs_heldout, y_pred_heldout = [], [], []\n",
        "\n",
        "  for i, batch in enumerate(heldout_dataloader):\n",
        "    X_batch = batch[\"data\"].to(device)\n",
        "    y_batch = batch[\"label\"].to(device)\n",
        "    \n",
        "    outputs = model_cnn(X_batch.float())\n",
        "    loss = loss_function(outputs, y_batch)\n",
        "    valid_loss.append(loss.item())\n",
        "\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    y_pred_heldout += predicted.cpu().numpy().tolist()\n",
        "    y_probs_heldout = torch.cat((y_probs_train, outputs), 0)\n",
        "    y_true_heldout += y_batch.cpu().numpy().tolist()\n",
        "y_probs_heldout = F.softmax(y_probs_train, dim=1).cpu().numpy()\n",
        "y_true_heldout = np.array(y_true_heldout)\n",
        "print(classification_report(y_true_heldout, y_pred_heldout))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.95      0.77      7741\n",
            "           1       0.41      0.23      0.30      7740\n",
            "           2       0.29      0.13      0.18      7741\n",
            "           3       0.36      0.22      0.28      7741\n",
            "           4       0.38      0.74      0.50      7740\n",
            "\n",
            "    accuracy                           0.45     38703\n",
            "   macro avg       0.42      0.45      0.41     38703\n",
            "weighted avg       0.42      0.45      0.41     38703\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwLh8OihJyZn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}